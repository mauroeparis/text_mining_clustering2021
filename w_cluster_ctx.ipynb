{"cells":[{"cell_type":"markdown","metadata":{},"source":[" # Text Mining Práctico Clustering\n"," [Consigna](https://sites.google.com/unc.edu.ar/textmining2021/pr%C3%A1ctico/clustering?authuser=0)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Setup inicial\n","import numpy as np\n","import pandas as pd\n","import plotly.express as px\n","import spacy\n","from sklearn import preprocessing\n","from sklearn.cluster import KMeans\n","from sklearn.feature_extraction import DictVectorizer\n","from sklearn.manifold import TSNE\n","from spacy.tokens import DocBin\n","from tqdm import tqdm\n","\n","\n","nlp = spacy.load(\"es_core_news_sm\")\n","nlp.Defaults.stop_words.add(\"e\")\n","nlp.Defaults.stop_words.add(\"y\")\n","nlp.Defaults.stop_words.add(\"a\")\n","nlp.Defaults.stop_words.add(\"o\")\n","nlp.Defaults.stop_words.add(\"u\")\n","nlp.Defaults.stop_words.add(\"o\")\n","nlp.Defaults.stop_words.add(\"etcétera\")\n","nlp.Defaults.stop_words.add(\"etc\")\n","\n","\n","DATASET_PATH = \"./lavoztextodump.txt\"\n","DOC_BIN_PATH = \"./articles_doc_bin.spacy\"\n","# 12936 es el número de documentos que vamos a generar.\n","TOTAL_ARTICLES = 12936\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Abrir dataset y crear librereia de objetos docs\n","# para no tener que crearla nuevamente sin necesidad.\n","# \n","# EN CASO DE TENER EL ARCHIVO:\n","# 'articles_doc_bin.spacy'\n","# ESTA CELDA NO ES NECESARIA\n","articles_doc_bin = DocBin()\n","\n","# Agregamos una barra de progreso tqdm\n","doc_bin_progress_bar = tqdm(total=TOTAL_ARTICLES)\n","with open(DATASET_PATH, \"r\") as dataset_file:\n","    while True:\n","        article_divider = dataset_file.readline()\n","        article_title = dataset_file.readline()\n","        article_content = dataset_file.readline()\n","        if not article_content:\n","            # Si no existe, llegamos al final del archivo\n","            break  # EOF\n","        # Agregar el objeto Doc compuesto por el titulo y contenido\n","        articles_doc_bin.add(\n","            nlp(\"{} {}\".format(article_title, article_content)))\n","        # Actializar barra de progreso\n","        doc_bin_progress_bar.update()\n","\n","articles_doc_bin.to_disk(DOC_BIN_PATH)\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Cargar archivo con los objetos Doc.\n","articles_doc_bin = DocBin().from_disk(DOC_BIN_PATH)\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Armar diccionario de tokens con features\n","words_feature_dict = dict()\n","\n","word_feature_dict_progress_bar = tqdm(total=TOTAL_ARTICLES)\n","for doc in articles_doc_bin.get_docs(nlp.vocab):\n","    sents = [sent for sent in doc.sents]\n","    for sent in sents:\n","        for token in sent:\n","            # Si la palabra es stopword o no está compuesta unicamente\n","            # por letras \n","            if token.is_stop or not token.is_alpha:\n","                continue\n","            # Obtener lemma de la palabra. Algunos lemmas son \"VERBO él\"\n","            # por lo cual vamos a tomar solo la parte del verbo.\n","            w_lemma = token.lemma_.split(\" \")[0]\n","            # Obtener features de la palabra o devolver un diccionario vacío\n","            word_feature_dict = words_feature_dict.get(w_lemma, {})\n","            ent_type = token.ent_type_ or \"NOENT\"\n","\n","            features = [\n","                \"POS__\" + token.pos_,\n","                \"DEP__\" + token.dep_,\n","                \"TAG__\" + token.tag_,\n","                \"LEMM_\" + w_lemma,\n","                \"HEAD_\" + token.head.lemma_.split(\" \")[0],\n","                \"ENT__\" + ent_type,\n","                \"count\",\n","            ]\n","            for f in features:\n","                # si la feature está definida y sumar uno\n","                # si la feature no está definida, devolver 0 y sumar 1\n","                word_feature_dict[f] = word_feature_dict.get(f, 0) + 1\n","            \n","            right_t = next(token.rights, None)\n","            if right_t and not right_t.is_punct and not right_t.is_stop:\n","                if right_t.is_alpha:\n","                    r_lemm = right_t.lemma_.split(\" \")[0]\n","                    feat_name = \"RLEM_\" + r_lemm\n","                    word_feature_dict[feat_name] = word_feature_dict.get(\n","                        feat_name, 0) + 1\n","                else:\n","                    r_lemm = \"NUM__\"\n","                    word_feature_dict[r_lemm] = word_feature_dict.get(\n","                        r_lemm, 0) + 1\n","\n","            left_ts = [t for t in token.lefts]\n","            if left_ts:\n","                left_t = left_ts[-1]\n","                if not left_t.is_punct and not left_t.is_stop:\n","                    if left_t.is_alpha:\n","                        l_lemm = left_t.lemma_.split(\" \")[0]\n","                        feat_name = \"LLEM_\" + l_lemm\n","                        word_feature_dict[feat_name] = word_feature_dict.get(\n","                            feat_name, 0) + 1\n","                    else:\n","                        l_lemm = \"NUM__\"\n","                        word_feature_dict[l_lemm] = word_feature_dict.get(\n","                            l_lemm, 0) + 1\n","            \n","            words_feature_dict[w_lemma] = word_feature_dict\n","\n","    word_feature_dict_progress_bar.update()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["filtered_words_feature_dict = dict()\n","\n","for w, f in words_feature_dict.items():\n","    if f[\"count\"] > 70:\n","        f.pop(\"count\")\n","        filtered_words_feature_dict[w] = f\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Crear lista con las features de cada token y\n","# un diccionario que guarde la posición de cada token dentro\n","# de la lista\n","words_feature_list = []\n","words_ids = {}\n","wid = 0\n","for word in filtered_words_feature_dict:\n","    if len(word) > 0:\n","        words_ids[word] = wid\n","        wid += 1\n","        words_feature_list.append(filtered_words_feature_dict[word])\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Utilizar DictVectorizer para crear una matriz \"scipy.sparse\"\n","# para utilizar los modelos de sklearn.\n","v = DictVectorizer(sparse=False)\n","matrix = v.fit_transform(words_feature_list)\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# normalizar la matriz\n","matrix_normed = preprocessing.normalize(matrix)\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Calcular varianza de cada columna\n","variances = np.square(matrix_normed).mean(axis=0) - \\\n","    np.square(matrix_normed.mean(axis=0))\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Quitar las columnas con poca varianza\n","threshold_v = 0.001\n","red_matrix = np.delete(matrix_normed, np.where(\n","    variances < threshold_v), axis=1)\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Proyectar matriz n dimensional en 2 dimensiones\n","tsne = TSNE(n_components=2, random_state=0)\n","matrix_dicc2d = tsne.fit_transform(red_matrix)\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Crear DataFrame que contenga token, su posición en x y en y\n","pointsspacy = pd.DataFrame(\n","    [\n","        (word, coords[0], coords[1])\n","        for word, coords in tqdm([\n","            (word, matrix_dicc2d[words_ids[word]])\n","            for word in words_ids\n","        ])\n","    ],\n","    columns=[\"word\", \"x\", \"y\"]\n",")\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Mostrar scatterplot de cada valor\n","fig_matrix = px.scatter(pointsspacy, x=\"x\", y=\"y\", hover_data=['word'])\n","fig_matrix.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Utilizamos Kmeans para crear clusters de palabras\n","kmeans = KMeans(n_clusters=6).fit(red_matrix)\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Creamos un DataFrame que contiene el token, las posiciones x e y y \n","# el cluster de cada token.\n","pointscluster = pd.DataFrame(\n","    [\n","        (word, coords[0], coords[1], cluster)\n","        for word, coords, cluster in tqdm([\n","            (word, matrix_dicc2d[words_ids[word]],\n","             kmeans.labels_[words_ids[word]])\n","            for word in words_ids\n","        ])\n","    ],\n","    columns=[\"word\", \"x\", \"y\", \"c\"]\n",")\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Mostrar scatterplot de cada valor y con diferentes colores de dependiendo\n","# del cluster al que pertenecen\n","fig_clusters = px.scatter(\n","    pointscluster, x=\"x\", y=\"y\", color=\"c\", hover_data=['word'])\n","fig_clusters.show()\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[""]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"orig_nbformat":4}}